{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7rWb/1NLqHQRTCLVDiJWT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safdarhayatawan/ATM-App/blob/main/pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AMMM8IRRd4K"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-pinecone pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddbxech5csy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H__0X6iZVBws",
        "outputId": "163992eb-df8d-4ffe-fb71-b2d621270684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google.colab import userdata\n",
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# # Retrieve Pinecone API Key\n",
        "# pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# # Debugging: Print API key to check if it's retrieved correctly (Remove in production)\n",
        "# print(\"Pinecone API Key:\", pinecone_api_key)  # Debugging: Ensure API key is retrieved\n",
        "\n",
        "# if not pinecone_api_key:\n",
        "#     raise ValueError(\"Pinecone API Key is missing. Please check if it is set properly.\")\n",
        "\n",
        "# # Initialize Pinecone\n",
        "# pc = Pinecone(api_key=pinecone_api_key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cOMgbIcbUXV",
        "outputId": "11570fd1-22c5-4dba-8862-f5eb1c726e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone API Key: pcsk_2XXaA9_GzwVNDcudmKVeCicVRMgrpfYgCg1nA71LcuNDAKgysapgZ19VHRWjqEGif2w5YT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Try fetching from userdata first\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n"
      ],
      "metadata": {
        "id": "dFlBrDj_cwEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "index_name = \"langchain-safdar-index\"  # Change if needed\n",
        "dimension = 768\n",
        "metric = \"cosine\"\n",
        "cloud = \"aws\"\n",
        "region = \"us-east-1\"\n",
        "\n",
        "# Get a list of existing indexes\n",
        "existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
        "\n",
        "if index_name in existing_indexes:\n",
        "    print(f\"Index '{index_name}' already exists. Connecting...\")\n",
        "else:\n",
        "    print(f\"Creating index: {index_name}...\")\n",
        "\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=metric,\n",
        "        spec=ServerlessSpec(cloud=cloud, region=region),\n",
        "    )\n",
        "\n",
        "    # Wait for the index to be created\n",
        "    while index_name not in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
        "        print(\"Waiting for index to be created...\")\n",
        "        time.sleep(2)\n",
        "\n",
        "    print(f\"Index '{index_name}' created successfully.\")\n",
        "\n",
        "# Connect to the existing or newly created index\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Connected to index: {index_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSSL6dHZesHZ",
        "outputId": "171583af-d0ca-47e7-e774-74568967547d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'langchain-safdar-index' already exists. Connecting...\n",
            "Connected to index: langchain-safdar-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "# userdata.get('GOOGLE_API_KEY')\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "E5Uap8Gi1Yif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"we are building a rag\")\n",
        "print(vector[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDboK8ztPOC4",
        "outputId": "055662d3-abf4-42f4-87d5-924e270e5918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.020280562341213226, -0.00720174890011549, -0.044231660664081573, -0.022845320403575897, -0.0029758561868220568]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "Tjy4iOnMQdNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have created your vector store, we can interact with it by adding and deleting different items.\n",
        "\n",
        "Add items to vector store\n",
        "We can add items to our vector store by using the add_documents function."
      ],
      "metadata": {
        "id": "BLtTGiaCebi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNvV6--IecgV",
        "outputId": "faa5a239-df96-4c50-b100-f369ff04cbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5857a5ca-cd28-4122-b6a7-02c40a49cddd',\n",
              " 'fdd0ff39-9e10-45b4-8a87-74f8dc2ed61b',\n",
              " 'f4a92d35-1d8a-4f89-884b-f39e7440f3e0',\n",
              " '522c9b30-3cf3-4ee0-b5db-ae91dfecf704',\n",
              " '9bda3367-4f53-436d-b338-91400baeb4c7',\n",
              " '6615a582-3fde-4711-8877-f5cfa231ed3c',\n",
              " '589e224e-e410-462c-876f-f6e1f491ce1e',\n",
              " 'b7b5d994-d31b-41c8-97c6-036ff3ffc615',\n",
              " 'b8f41778-55c3-41ee-8867-2bee72fd880b',\n",
              " 'd99e25d2-bb21-4b34-b57b-6c055f5cbd1a']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\"\n",
        "    # k=2,\n",
        "    # filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRvBTXZejhby",
        "outputId": "4c66ebd7-eb59-4d99-c26b-cb5b34c551fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* I have a bad feeling I am going to get deleted :( [{'source': 'tweet'}]\n",
            "* I had chocalate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHZnxBQ-kgB0",
        "outputId": "bdf3a40d-16ce-4a9a-b0af-3ba65b8024e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# // ... existing code ...\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "# Initialize Google's Generative AI model\n",
        "model = GoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
        ")\n",
        "\n",
        "def answer_user(query: str):\n",
        "    # Get relevant documents\n",
        "    results = vector_store.similarity_search_with_score(query, k=2)\n",
        "\n",
        "    # Format the context from retrieved documents\n",
        "    context = \"\\n\".join([f\"Document {i+1}: {doc.page_content}\"\n",
        "                        for i, (doc, score) in enumerate(results)])\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"Based on the following context, answer the query.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Get response from the model\n",
        "    response = model.invoke(prompt)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"What's the weather forecast for tomorrow?\"\n",
        "print(answer_user(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7skIwGTbldW9",
        "outputId": "1385c75c-a3d7-4e28-ee4f-59c36ca4b09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloudy and overcast, with a high of 62 degrees.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2HbxJd3brDIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "93EqK-AlrG4L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4vA7YmYrPBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}